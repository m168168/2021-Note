贪心-机器学习

## 1: 计算文档的相识度

W MD ： 

EMD距离： 本质是最优化运输问题

2: 合叶损失函数。logsti损失函数

合叶损失函数 都所有数据都是平等的，不会某个点剧烈波动

凸函数 容易优化

在自变量等于1 的时候，不可导，需要分段求倒

自变量大于0， 值为0 ， 这个数据就不用优化了

小于0的部分梯度比较小，对错误分类的惩罚比较轻

只保存支持向量

核函数：

将特征映射到更高的维度

核函数成立的条件：

Gram矩阵： 

对称矩阵

半正定矩阵

多项式核

高斯核 特征需要正规化

线性回归：

Boxcox变化：

对数变化



## 2: basic  expansion 数据扩增

x x^2  x^3



模式的bias 和virance  (偏差和方差)

bias：期望值与真实的差异

随着模型的越来越复杂， 预测值与真实值越来越接近， 偏差减小。

variance： 在一个特定的数据集上训练的模型与所有模型的期望差异

衡量模型对特定的数据集变化的敏感度

随着模型的复杂度增加，Variance 增加（预测值 分布比较开 ， 最大与最小值差异大）

调参  w1, w2,...wn.    是每个变量的参数 ，可以表示变量之间的相关性，如果某个w= 0,表示对应的变量x,都model无影响



正则化  Regularization  term 

 Loss = loss + regularization term 

L1   L2 

L1, L2 ,求倒都指向圆心 ， Loss+ L正则化项， 让求倒数的时候，向圆心靠拢。

L1的特点：

不容易计算，在0点连续但是不可导，需要分段求导

L1模型可以将一些权值缩小到0 ，意味着该变量对结果影响为0 （建模特征数量巨大的时首选）相当于特征筛选

对异常值更具抵抗力

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210721225728378.png" alt="image-20210721225728378" style="zoom:50%;" />

L2正则化的特点：

容易计算，适合梯度求导

对异常值敏感

相对L1更加精确

相关的预测特征对应的系数相似



<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210721225956107.png" alt="image-20210721225956107" style="zoom:50%;" />

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210721230030106.png" alt="image-20210721230030106" style="zoom:50%;" />

![image-20210721231000020](/Users/mrwang/Library/Application Support/typora-user-images/image-20210721231000020.png)

## 3: 什么是梯度下降算法：

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210721232558837.png" alt="image-20210721232558837" style="zoom:80%;" />

BGD： batch-gradient-descent

![image-20210721232733311](/Users/mrwang/Library/Application Support/typora-user-images/image-20210721232733311.png)

SGD：

![image-20210721232832031](/Users/mrwang/Library/Application Support/typora-user-images/image-20210721232832031.png)

Mini-batch-size:

![image-20210721232953455](/Users/mrwang/Library/Application Support/typora-user-images/image-20210721232953455.png)

## 4:模型评估的方法：

对样本集的划分：

1: hold-out  对样本数据集适当的划分， 训练集 和测试集（可以采用分层抽样的方法，保持训练集和测试集数据分布一致）

2:自助发： bootstrapping  有放回的抽样

3: 交叉验证 cross- validation 

模型的参数：

算法的参数： 超参数 数量少，人工设定

模型的参数： 数量多，学习得到

## 5: 数据降维 PCA LDA 

均值 ，方差 ， 协方差 

协方差是衡量两个维度之间的相关性

p c a : 

每一行均值化处理 ， 然后求特征值，特征值排序，求前k个最大（降维为k）





<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210722230148221.png" alt="image-20210722230148221" style="zoom:80%;" />

LDA : 

可以降维

可以分类

## 6: Ensebel mode

bagging：  

Randforest:

decesion Treee:

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210728230627642.png" alt="image-20210728230627642" style="zoom:50%;" />

random foremost  

dropout

输入输出加噪声

Boosting ：

GBDT ， XG boost，Adaboost(调节样本权重)

为什么集成模型会降低错误率？

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210727230645125.png" alt="image-20210727230645125" style="zoom:50%;" />

7个专家 做预测 每个人犯错误的概率 0.3 ， 多数投票 ，最后犯错的概率0.1，所以多个人做决策犯错误的概率减减

小



1: Majorit Vote 投票

2： Bagging  (bootstrap采样)

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210722232312633.png" alt="image-20210722232312633" style="zoom:80%;" />





3:  Boosting ： AdaBoost VS. Gradient Boost

主要区别是如何训练一个模型，如何集成模型

AdaBoost: 

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210728230239420.png" alt="image-20210728230239420" style="zoom:80%;" />

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210728230347782.png" alt="image-20210728230347782" style="zoom:80%;" />



Boosting: 是对权重的调整





bagging ： 随机森林 ，对模型的集成 也可以是不同数据

Gradient Boost （ Light GBM ， XG Boost ，） 残差

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210727233030930.png" alt="image-20210727233030930" style="zoom:50%;" />

4: Stacking

![image-20210727224755518](/Users/mrwang/Library/Application Support/typora-user-images/image-20210727224755518.png)

决策树：

需要学习的3样东西：

学习树的形状

每一个决策的阈值

叶节点的值

随机森林：

只有一分训练数据 boost rap采用

确保多棵决策树优于单颗决策树

稳定性基础是基础是多样性（数据随机采用，特征随机采样）

D PP （Regularrzaion）

<img src="/Users/mrwang/Library/Application Support/typora-user-images/image-20210727231457332.png" alt="image-20210727231457332" style="zoom:50%;" />

树的复杂度：

叶节点的个数

树的深度

叶子节点值
